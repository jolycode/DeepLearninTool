{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "torch.cuda.empty_cache()\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from termcolor import colored\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast as T5Tokenizer\n",
    ")\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t5_eng = pickle.load(open('modeltest4.pkl', 'rb'))  \n",
    "tokenizer_t5_eng = pickle.load(open('tokenizer.pkl', 'rb'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-xsum-6-6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('tokenizerabout2.pickle', 'rb') as handle:\n",
    "#    tokenizerabout = pickle.load(handle)\n",
    "#modelabout = keras.models.load_model('modelabout2.h5')\n",
    "\n",
    "#with open('tokenizerposneg2.pickle', 'rb') as handle:\n",
    "#   tokenizerposs = pickle.load(handle)\n",
    "#modelposs = keras.models.load_model('modelposneg2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizerabout = pickle.load(handle)\n",
    "modelabout = keras.models.load_model('modelabout2.h5')\n",
    "\n",
    "with open('tokenizerposneg.pickle', 'rb') as handle:\n",
    "    tokenizerposs = pickle.load(handle)\n",
    "modelposs = keras.models.load_model('modelposneg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_summary_model__t5_eng(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_t5_eng = pickle.load(open('modeltest4.pkl', 'rb'))  \n",
    "        self.tokenizer_t5_eng = pickle.load(open('tokenizer.pkl', 'rb'))  \n",
    "    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None):\n",
    "        output = self.model_t5_eng(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=decoder_attention_mask\n",
    "        )\n",
    "\n",
    "        return output.loss, output.logits\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        max_length,\n",
    "        num_beams=2,\n",
    "        repetition_penalty=2.5,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True,\n",
    "        decoder_start_token_id=None\n",
    "    ):\n",
    "        generated_ids = self.model_t5_eng.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            early_stopping=early_stopping,\n",
    "            decoder_start_token_id=decoder_start_token_id\n",
    "        )\n",
    "        preds = []\n",
    "        for gen_id in generated_ids:\n",
    "            preds.append(tokenizer_t5_eng.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "\n",
    "        return \"\".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summ_text_main(text,len_penalty,max_len):\n",
    "    model_t5_eng = my_summary_model__t5_eng()\n",
    "    summary = model_t5_eng.generate(\n",
    "        input_ids=tokenizer_t5_eng(text, max_length=1024, padding='max_length', truncation=True, return_attention_mask=True, add_special_tokens=True, return_tensors='pt')['input_ids'],\n",
    "        attention_mask=tokenizer_t5_eng(text, max_length=1024, padding='max_length', truncation=True, return_attention_mask=True, add_special_tokens=True, return_tensors='pt')['attention_mask'],\n",
    "        max_length=max_len,\n",
    "        num_beams=2,\n",
    "        repetition_penalty=2.5,\n",
    "        length_penalty=len_penalty,\n",
    "        early_stopping=True,\n",
    "        decoder_start_token_id=0\n",
    "    ) \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whatabout(news_article):\n",
    "    class_labels = ['World', 'Sports', 'Business', 'Science/Technology']\n",
    "    news_article_sequence = tokenizerabout.texts_to_sequences([news_article])\n",
    "    news_article_sequence = pad_sequences(news_article_sequence, maxlen=170)\n",
    "    predictions = modelabout.predict(news_article_sequence, verbose=0)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "    predicted_probability = predictions[0][predicted_class]\n",
    "    predicted_class_label = class_labels[predicted_class]\n",
    "    predicted_percentage = \"{:.2f}\".format(predicted_probability * 100)\n",
    "    print(\"Relevance:\")\n",
    "    print(f\"Probability: \" +predicted_percentage+ \"% \" + \" Predicted Class:\", predicted_class_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance:\n",
      "Probability: 100.00%  Predicted Class: Science/Technology\n",
      "<function whatabout at 0x000001BED80DFD30>\n"
     ]
    }
   ],
   "source": [
    "news_article = \"The tea stall in Guajarat's Vadnagar where Prime Minister Narendra Modi once sold tea during his childhood is all set to become a tourist spot with the Centre deciding to give it a face-lift. The stall is located on one of the platforms of the Vadnagar Railway station. Converting the stall into a tourist spot is part of a larger project of putting Modi's birthplace Vadnagar in Mehsana district of Gujarat on the world tourism map. Officials of the Ministry of Culture and Tourism and Archaeological Survey of India (ASI) visited the town yesterday. The team of officials was led by Union Culture Mahesh Sharma, who later announced that the original charm of the tea stall will be preserved while giving it a modern touch. Apart from being the birthplace of our PM, Vadnagar is an important historical centre having famous Sharmishta Lake and a step-well. The ASI had recently found remains of a Buddhist Monastery during excavation, which is still going on, Sharma told reporters at Gandhinagar yesterday. PRESERVING THE ORIGINAL CHARM Ahead of the 2014 Lok Sabha elections, Modi had often mentioned that he used to sell tea during his childhood at the Vadnagar railway station along with his father.Inside the Vadnagar railway station, there is a small tea stall, from where our PM had probably started his life's journey. We also want to develop that tea stall as a tourism spot. We will try to preserve the original charm of the tea stall while giving it a modern touch. Our aim is to put Vadnagar on world tourism map, Sharma said. Earlier, Divisional Railway Manager (DRM) of Ahmedabad division Dinesh Kumar had said the entire project of developing Vadnagar and adjoining places in Mehsana district would cost over Rs 100 crore.The development of Vadnagar railway station is one of the components of the Rs 100 crore project to develop Vadnagar, Modhera and Patan as tourist destinations. As of now, the Ministry of Tourism has given Rs eight crore to the state Tourism Department to develop the railway station,Kumar had said.\"\n",
    "whatabout(news_article)\n",
    "print(whatabout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posneg(news_article):\n",
    "    class_labels = ['Irrelevant', 'Negative', 'Positive', 'Neutral']\n",
    "    news_article_sequence = tokenizerposs.texts_to_sequences([news_article])\n",
    "    news_article_sequence = pad_sequences(news_article_sequence, maxlen=100)\n",
    "    predictions = modelposs.predict(news_article_sequence, verbose=0)\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    predicted_class_label = class_labels[predicted_class_index]\n",
    "    predicted_probability = predictions[0][predicted_class_index]\n",
    "    predicted_percentage = \"{:.2f}\".format(predicted_probability * 100)\n",
    "    print(\"Polarity:\")\n",
    "    print(\"Predicted Probability:\", predicted_percentage, \"%\" + \" Predicted Class:\", predicted_class_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity:\n",
      "Predicted Probability: 66.09 % Predicted Class: Irrelevant\n",
      "<function posneg at 0x000001BED8394040>\n"
     ]
    }
   ],
   "source": [
    "news_article =\"A 38-year-old woman in the US, who was apprehended twice for allegedly trying to jump the White House fence last week, has been arrested for scaling a fence at the Treasury Building next to the White House.\"\n",
    "\n",
    "posneg(news_article)\n",
    "print(posneg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import time\n",
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "from tkinter import messagebox\n",
    "def on_scale_change(event):\n",
    "    global lenofsum\n",
    "    global maxlen\n",
    "    global minlensum\n",
    "    global maxlensum\n",
    "    new_value = scale.get()\n",
    "    lenofsum=0.5\n",
    "    maxlen=50\n",
    "    minlensum=10\n",
    "    maxlensum=50\n",
    "    if new_value == default_value:\n",
    "        return  \n",
    "    elif new_value != default_value:\n",
    "        if new_value <= 50:\n",
    "            lenofsum = 0.5\n",
    "            maxlen = 50\n",
    "            minlensum = 10\n",
    "            maxlensum = 50\n",
    "            return \n",
    "        elif 51 <= new_value <= 75:\n",
    "            lenofsum = 2.00\n",
    "            maxlen = 100\n",
    "            minlensum = 25\n",
    "            maxlensum = 90\n",
    "            return\n",
    "        else:\n",
    "            lenofsum = 3.00\n",
    "            maxlen = 150\n",
    "            minlensum = 45\n",
    "            maxlensum = 150\n",
    "            return\n",
    "is_on = False\n",
    "def whatabout(news_article):\n",
    "    start_time = time.time()\n",
    "    class_labels = ['World', 'Sports', 'Business', 'Science/Technology']\n",
    "    news_article_sequence = tokenizerabout.texts_to_sequences([news_article])\n",
    "    news_article_sequence = pad_sequences(news_article_sequence, maxlen=170)\n",
    "    predictions = modelabout.predict(news_article_sequence, verbose=0)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "    predicted_probability = predictions[0][predicted_class]\n",
    "    predicted_percentage = \"{:.2f}\".format(predicted_probability * 100)\n",
    "    predicted_class_label = class_labels[predicted_class]\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    summary.insert('1.0', f\"Relevance: \\n\" +\n",
    "               \"Predicted Probability: \" + predicted_percentage + \" %\" +\n",
    "               \" Predicted Class: \" + predicted_class_label +\n",
    "               \" Execution time: {:.5f}\".format(execution_time) + \" seconds\\n\")\n",
    "    \n",
    "def posneg(news_article):\n",
    "    start_time = time.time()\n",
    "    class_labels = ['Irrelevant', 'Negative', 'Positive', 'Neutral']\n",
    "    news_article_sequence = tokenizerposs.texts_to_sequences([news_article])\n",
    "    news_article_sequence = pad_sequences(news_article_sequence, maxlen=170)\n",
    "    predictions = modelposs.predict(news_article_sequence, verbose=0)\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    predicted_class_label = class_labels[predicted_class_index]\n",
    "    predicted_probability = predictions[0][predicted_class_index]\n",
    "    predicted_percentage = \"{:.2f}\".format(predicted_probability * 100)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    summary.insert('1.0', f\"Polarity: \\n\" +\n",
    "               \"Predicted Probability: \" + predicted_percentage + \" %\" +\n",
    "               \" Predicted Class: \" + predicted_class_label +\n",
    "               \" Execution time: {:.5f}\".format(execution_time) + \" seconds\\n\")\n",
    "def onoff():\n",
    "    global is_on\n",
    "    if is_on:\n",
    "        on_button.config(image=off)\n",
    "        is_on = False\n",
    "    else:\n",
    "        on_button.config(image=on)\n",
    "        is_on = True\n",
    "\n",
    "def summarize():\n",
    "    global is_on\n",
    "    global lenofsum\n",
    "    global maxlen\n",
    "    global minlensum\n",
    "    global maxlensum\n",
    "    input_text = intext.get('1.0', tk.END) \n",
    "    if input_text.strip() == \"\":\n",
    "        return\n",
    "    else:\n",
    "        if is_on == False:\n",
    "            start_time = time.time()\n",
    "            textout = summ_text_main(input_text, lenofsum, maxlen)\n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            input_word_count = len(input_text.split())\n",
    "            textout_word_count = len(textout.split())\n",
    "            summary.config(state=tk.NORMAL, bg='white', fg='black')\n",
    "            summary.delete('1.0', tk.END)\n",
    "            summary.insert(tk.END, f\"Summarize: {textout}\\n\")\n",
    "            summary.insert(tk.END, f\"Execution time: {execution_time:.5f} seconds\\n\")\n",
    "            summary.insert(tk.END, f\"Original Word Count: {input_word_count}\\n\")\n",
    "            summary.insert(tk.END, f\"Summarized Word Count: {textout_word_count}\\n\")\n",
    "            posneg(input_text)\n",
    "            whatabout(input_text)\n",
    "            summary.config(bg='#dddddd')\n",
    "\n",
    "        if is_on == True:\n",
    "            start_time = time.time()\n",
    "            summarybert = summarizer(input_text, max_length=maxlensum, min_length=minlensum, do_sample=False)\n",
    "            summout = summarybert[0]['summary_text']\n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            summary.config(state=tk.NORMAL, bg='white', fg='black')\n",
    "            summary.delete('1.0', tk.END)\n",
    "            summary.insert('1.0', f\"Summarize: {summout}\")\n",
    "            execution_time_formatted = \"{:.1f}\".format(execution_time)\n",
    "            summary.insert(tk.END, f\"\\nExecution time: {execution_time_formatted} seconds\")\n",
    "            input_word_count = len(input_text.split())\n",
    "            summout_word_count = len(summout.split())\n",
    "            summary.insert(tk.END, f\"\\nOriginal Word Count: {input_word_count}\")\n",
    "            summary.insert(tk.END, f\"\\nSummarized Word Count: {summout_word_count}\")\n",
    "            posneg(input_text)\n",
    "            whatabout(input_text)\n",
    "            summary.config(bg='#dddddd')\n",
    "\n",
    "\n",
    "def clearup():#botton to clear input and summ boxes\n",
    "    intext.delete(\"1.0\", tk.END)\n",
    "    summary.delete(\"1.0\", tk.END)\n",
    "def copysum():#coppy the summ in the box\n",
    "    summary.clipboard_clear()\n",
    "    summary.clipboard_append(summary.get(\"1.0\", tk.END))\n",
    "def dowloadsum():#dowloadıng summ as txt\n",
    "    output_str = summary.get('1.0', tk.END)\n",
    "    if output_str.strip() == \"\":\n",
    "        return\n",
    "    file_path = filedialog.asksaveasfilename(defaultextension=\".txt\")\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(output_str)\n",
    "\n",
    "def exitfrom():#exit botton\n",
    "    root.destroy()\n",
    "def show_about():#info about me\n",
    "    messagebox.showinfo(\"About\", \"Deep learning summarization tool \\nMade by Danylo Fedorov \\n\")\n",
    "def select_file():#menu bottun to select txt files insides text\n",
    "    file_path = filedialog.askopenfilename(defaultextension=\".txt\", filetypes=[(\"Text Files\", \"*.txt\")])\n",
    "    if file_path:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            content = file.read()\n",
    "            intext.delete(1.0, tk.END)\n",
    "            intext.insert(tk.END, content)\n",
    "\n",
    "root = tk.Tk()\n",
    "\n",
    "original_text = \" \"\n",
    "root.title(\"AI\")\n",
    "root.geometry('1600x900')\n",
    "inputlabel=tk.Label(root,text=\"Input:\")\n",
    "inputlabel.pack()\n",
    "intext = tk.Text(root,height=25,width=140)\n",
    "intext.pack()\n",
    "slabel = tk.Label(root,text=\"Summary:\")\n",
    "slabel.pack()\n",
    "summary=tk.Text(root,height=15,width=140)\n",
    "summary.config(bg='#dddddd')\n",
    "summary.pack()\n",
    "menu_bar = tk.Menu(root)\n",
    "file_menu = tk.Menu(menu_bar, tearoff=0)\n",
    "file_menu.add_command(label=\"Open\", command=select_file)\n",
    "file_menu.add_separator()\n",
    "file_menu.add_command(label=\"Exit\", command=root.destroy)\n",
    "menu_bar.add_cascade(label=\"File\", menu=file_menu)\n",
    "help_menu = tk.Menu(menu_bar, tearoff=0)\n",
    "help_menu.add_command(label=\"About\", command=show_about)\n",
    "menu_bar.add_cascade(label=\"Help\", menu=help_menu)\n",
    "root.config(menu=menu_bar)\n",
    "\n",
    "\n",
    "default_value = 0\n",
    "scale = tk.Scale(root, from_=0, to=100, orient=tk.HORIZONTAL, length=100, showvalue=0)\n",
    "scale.set(default_value)\n",
    "scale.place(x=1415, y=20)\n",
    "scale.bind(\"<ButtonRelease-1>\", on_scale_change)\n",
    "scale2 = tk.Label(root,text=\"Summary Length:\")\n",
    "scale2.place(x=1415, y=2)\n",
    "scale3 = tk.Label(root,text=\"∘Short\")\n",
    "scale3.place(x=1375, y=18)\n",
    "scale4 = tk.Label(root,text=\"∘Long\")\n",
    "scale4.place(x=1520, y=18)\n",
    "#bottons\n",
    "btn=tk.Button(root,text=\"Summarize\",command=summarize)\n",
    "btn.place(x=580, y=690)\n",
    "clr=tk.Button(root,text=\" Clear \",command=clearup)\n",
    "clr.place(x=655, y=690)\n",
    "ext=tk.Button(root,text=\" Dowload \",command=dowloadsum)\n",
    "ext.place(x=705, y=690)\n",
    "ext=tk.Button(root,text=\" Copy \",command=copysum)\n",
    "ext.place(x=775, y=690)\n",
    "ext=tk.Button(root,text=\" Exit \",command=exitfrom)\n",
    "ext.place(x=825, y=690)\n",
    "\n",
    "on = PhotoImage(file=\"A.png\")\n",
    "off = PhotoImage(file=\"B.png\")\n",
    "on_button = Button(root, image=off, bd=0, command=onoff)\n",
    "on_button.place(x=1455, y=55)\n",
    "\n",
    "scale3 = tk.Label(root,text=\"∘Abstractive\")\n",
    "scale3.place(x=1375, y=55)\n",
    "scale4 = tk.Label(root,text=\"∘Extractive\")\n",
    "scale4.place(x=1520, y=55)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
